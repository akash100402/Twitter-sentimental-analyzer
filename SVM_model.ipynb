{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "28dd366a-de8f-4c51-818a-637c92fe07f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://medium.com/@sacbis9/itsm-open-active-incidents-closing-prediction-incident-management-bdf1684d84dc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d848355d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# basic libs\n",
    "import re\n",
    "from string import punctuation\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# sklearn\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# NLP\n",
    "import nltk\n",
    "import spacy\n",
    "import emoji\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import joblib  # or import pickle\n",
    "\n",
    "# sentence-transformers\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# local exception handler (keep if you have this module)\n",
    "# from src.exception_handler import handle_exception\n",
    "\n",
    "# load models / resources\n",
    "# NOTE: make sure the spacy model 'en_core_web_sm' is installed in the environment.\n",
    "nlp = spacy.load(\"en_core_web_sm\", disable=['parser', 'ner'])  # faster if parser+ner unused\n",
    "en = spacy.load(\"en_core_web_sm\")  # if you need full pipeline elsewhere\n",
    "\n",
    "# sentence-transformer embedder (device can be \"cpu\" or \"cuda\")\n",
    "embedder = SentenceTransformer('all-MiniLM-L6-v2', device=\"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6dafdb81",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Akash\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Akash\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# NLTK resources (run once)\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "# optionally:\n",
    "# nltk.download('wordnet')\n",
    "# nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6cfb9e22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data before dropping duplicates: (10000, 2)\n",
      "Test data before dropping duplicates: (4913, 2)\n",
      "Train data after dropping duplicates: (9591, 2)\n",
      "Test data after dropping duplicates: (4809, 2)\n"
     ]
    }
   ],
   "source": [
    "train_data_path = \"./data/train_sentiment_tweet.csv\"\n",
    "test_data_path = \"./data/test_sentiment_tweet.csv\"\n",
    "train_data = pd.read_csv(train_data_path)\n",
    "test_data = pd.read_csv(test_data_path)\n",
    "# normalize column names\n",
    "for i in [train_data, test_data]:\n",
    "    i.columns = i.columns.str.lower().str.replace(\" \", \"_\")\n",
    "\n",
    "    \n",
    "\n",
    "# ✅ Drop duplicates\n",
    "print(\"Train data before dropping duplicates:\", train_data.shape)\n",
    "print(\"Test data before dropping duplicates:\", test_data.shape)\n",
    "train_data = train_data.drop_duplicates(keep=\"first\")\n",
    "test_data = test_data.drop_duplicates(keep=\"first\")\n",
    "print(\"Train data after dropping duplicates:\", train_data.shape)\n",
    "print(\"Test data after dropping duplicates:\", test_data.shape)\n",
    "\n",
    "# ✅ Reset index\n",
    "train_data = train_data.reset_index(drop=True)\n",
    "test_data = test_data.reset_index(drop=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a72d2baf-3c5c-4f86-96bb-bf1546f202a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import emoji\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "def clean_html_data_in_string(input_string: str) -> str:\n",
    "    \"\"\"\n",
    "    Cleans HTML tags from the input string.\n",
    "\n",
    "    Args:\n",
    "        input_string (str): The string containing HTML data.\n",
    "\n",
    "    Returns:\n",
    "        str: The string with HTML content removed and only text retained.\n",
    "    \"\"\"\n",
    "    soup = BeautifulSoup(input_string, \"html.parser\")\n",
    "    return soup.get_text()\n",
    "\n",
    "\n",
    "def remove_email_n_url(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Remove URLs, emails, and domain-like patterns from text.\n",
    "\n",
    "    Args:\n",
    "        text (str): Input text.\n",
    "\n",
    "    Returns:\n",
    "        str: Cleaned text without URLs/emails.\n",
    "    \"\"\"\n",
    "    text = re.sub(\n",
    "        r\"https?://\\S+|www\\.\\S+|\\S+@\\S+|[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}\",\n",
    "        \"\",\n",
    "        text,\n",
    "    )\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    return text\n",
    "\n",
    "\n",
    "def give_emoji_free_text(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Remove emojis and emoticons from text.\n",
    "\n",
    "    Args:\n",
    "        text (str): Input text.\n",
    "\n",
    "    Returns:\n",
    "        str: Cleaned text without emojis/emoticons.\n",
    "    \"\"\"\n",
    "    # Remove emojis using the emoji library\n",
    "    text = emoji.replace_emoji(text, replace=\"\")\n",
    "\n",
    "    # Define regex for common emoticons\n",
    "    emoticon_pattern = r\"\"\"\n",
    "        (?:\n",
    "          [<>]?\n",
    "          [:;=8]                     # eyes\n",
    "          [\\-o\\*\\']?                  # optional nose\n",
    "          [\\)\\]\\(\\[dDpP/\\:\\}\\{@\\|\\\\]  # mouth\n",
    "        )\n",
    "        |\n",
    "        (?:\n",
    "          [\\)\\]\\(\\[dDpP/\\:\\}\\{@\\|\\\\]  # mouth\n",
    "          [\\-o\\*\\']?                  # optional nose\n",
    "          [:;=8]                      # eyes\n",
    "          [<>]?\n",
    "        )\n",
    "    \"\"\"\n",
    "    text = re.sub(emoticon_pattern, \"\", text, flags=re.VERBOSE)\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    return text\n",
    "\n",
    "\n",
    "def removing_punctuation(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Remove punctuation and underscores from the input text.\n",
    "\n",
    "    Args:\n",
    "        text (str): Input text.\n",
    "\n",
    "    Returns:\n",
    "        str: Cleaned text without punctuation/underscores.\n",
    "    \"\"\"\n",
    "    # Remove punctuation (except underscore)\n",
    "    text = re.sub(r\"[^\\w\\s]\", \"\", text)\n",
    "    # Replace underscores with space\n",
    "    text = text.replace(\"_\", \" \")\n",
    "    # Clean up spaces\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    return text\n",
    "\n",
    "\n",
    "def rem_numbers(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Remove all numeric digits from the input text.\n",
    "    \"\"\"\n",
    "    return re.sub(r\"\\d+\", \"\", text)\n",
    "\n",
    "\n",
    "def removing_whitespaces(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Normalize multiple whitespaces into a single space.\n",
    "    \"\"\"\n",
    "    return re.sub(r\"\\s+\", \" \", text).strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9755487e-d40d-45f0-8120-55fb40377448",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def tokenization(text: str, as_string: bool = False):\n",
    "    \"\"\"\n",
    "    Tokenize the input text using Spacy.\n",
    "\n",
    "    Args:\n",
    "        text (str): The text to tokenize.\n",
    "        as_string (bool): If True, returns tokens as a single string joined by spaces.\n",
    "                          If False, returns a list of tokens.\n",
    "\n",
    "    Returns:\n",
    "        list[str] | str: Tokenized text.\n",
    "    \"\"\"\n",
    "    doc = nlp(text)\n",
    "    tokens = [token.text for token in doc]\n",
    "\n",
    "    return \" \".join(tokens) if as_string else tokens\n",
    "\n",
    "\n",
    "def rem_stop_words(word_list):\n",
    "    \"\"\"\n",
    "    Remove stop words from a list of words using Spacy's stop words list.\n",
    "\n",
    "    Args:\n",
    "        word_list (list[str]): List of words.\n",
    "\n",
    "    Returns:\n",
    "        str: Filtered words joined by spaces.\n",
    "    \"\"\"\n",
    "    sw_list = nlp.Defaults.stop_words\n",
    "    filtered_words = [word for word in word_list if word.lower() not in sw_list]\n",
    "    return \" \".join(filtered_words)\n",
    "\n",
    "\n",
    "def lemmatization(text: str):\n",
    "    \"\"\"\n",
    "    Lemmatizes the input text using Spacy.\n",
    "\n",
    "    Args:\n",
    "        text (str): The input text.\n",
    "\n",
    "    Returns:\n",
    "        list[str]: A list of lemmatized words.\n",
    "    \"\"\"\n",
    "    doc = nlp(text)\n",
    "    return [token.lemma_ for token in doc]\n",
    "\n",
    "\n",
    "def remove_short_words(text: str):\n",
    "    \"\"\"\n",
    "    Removes words with fewer than 3 characters.\n",
    "\n",
    "    Args:\n",
    "        text (str): The input text.\n",
    "\n",
    "    Returns:\n",
    "        str: Text with short words removed.\n",
    "    \"\"\"\n",
    "    return \" \".join(word for word in text.split() if len(word) > 2)\n",
    "\n",
    "\n",
    "def sentence_transformation(corpus: list[str]) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Encode sentences into embeddings.\n",
    "\n",
    "    Args:\n",
    "        corpus (list[str]): A list of sentences.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Sentence embeddings.\n",
    "    \"\"\"\n",
    "    embeddings = embedder.encode(corpus)\n",
    "    return pd.DataFrame(embeddings)\n",
    "\n",
    "\n",
    "def preprocess_test_data(data: pd.DataFrame, required_columns: list[str]) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Preprocess test data:\n",
    "    - Merge required columns\n",
    "    - Clean text\n",
    "    - Tokenize, remove stopwords, lemmatize, etc.\n",
    "    - Return sentence embeddings as DataFrame.\n",
    "\n",
    "    Args:\n",
    "        data (pd.DataFrame): Input data.\n",
    "        required_columns (list[str]): Columns to merge into text.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Embeddings.\n",
    "    \"\"\"\n",
    "    df = data.copy()\n",
    "\n",
    "    # Merge columns into single text field\n",
    "    try:\n",
    "        df[\"text\"] = df[required_columns].astype(str).apply(lambda x: \" \".join(x), axis=1)\n",
    "    except Exception as e:\n",
    "        raise ValueError(f\"Error while merging columns into text: {e}\")\n",
    "\n",
    "    try:\n",
    "        df[\"text\"] = df[\"text\"].apply(clean_html_data_in_string)\n",
    "        df[\"text\"] = df[\"text\"].apply(remove_email_n_url)\n",
    "        df[\"text\"] = df[\"text\"].apply(give_emoji_free_text)\n",
    "        df[\"text\"] = df[\"text\"].apply(removing_punctuation)\n",
    "        df[\"text\"] = df[\"text\"].apply(rem_numbers)\n",
    "        df[\"text\"] = df[\"text\"].apply(removing_whitespaces)\n",
    "        df[\"text\"] = df[\"text\"].apply(tokenization)  # returns list\n",
    "        df[\"text\"] = df[\"text\"].apply(rem_stop_words)\n",
    "        df[\"text\"] = df[\"text\"].apply(lemmatization).apply(lambda x: \" \".join(x))\n",
    "        df[\"text\"] = df[\"text\"].apply(remove_short_words)\n",
    "        df[\"text\"] = df[\"text\"].apply(str.strip).apply(str.lower)\n",
    "    except Exception as e:\n",
    "        raise ValueError(f\"Error during preprocessing: {e}\")\n",
    "\n",
    "    # Generate embeddings\n",
    "    try:\n",
    "        bert_df = sentence_transformation(df[\"text\"].fillna(\"\").reset_index(drop=True).tolist())\n",
    "    except Exception as e:\n",
    "        raise ValueError(f\"Error during sentence transformation: {e}\")\n",
    "\n",
    "    return bert_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e0cc0717",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "SVC.__init__() got an unexpected keyword argument 'max_depth'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[19]\u001b[39m\u001b[32m, line 17\u001b[39m\n\u001b[32m     15\u001b[39m \u001b[38;5;66;03m# 3. Initialize and train the SVM model on TRAINING data\u001b[39;00m\n\u001b[32m     16\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msvm\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SVC\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m svm_model = \u001b[43mSVC\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     18\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# random_state=42, \u001b[39;49;00m\n\u001b[32m     19\u001b[39m \n\u001b[32m     20\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m42\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     21\u001b[39m \u001b[43m     \u001b[49m\u001b[43mkernel\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mlinear\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     22\u001b[39m \n\u001b[32m     23\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# n_estimators=100,\u001b[39;49;00m\n\u001b[32m     24\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_depth\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Let trees grow deep\u001b[39;49;00m\n\u001b[32m     25\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmin_samples_split\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m   \u001b[49m\u001b[38;5;66;43;03m# Minimum samples to split a node\u001b[39;49;00m\n\u001b[32m     26\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmin_samples_leaf\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# Minimum samples at a leaf node\u001b[39;49;00m\n\u001b[32m     27\u001b[39m \u001b[43m    \u001b[49m\u001b[43mclass_weight\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mbalanced\u001b[39;49m\u001b[33;43m'\u001b[39;49m\n\u001b[32m     28\u001b[39m \u001b[43m                \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Changed to SVC with linear kernel\u001b[39;00m\n\u001b[32m     29\u001b[39m svm_model.fit(X_train, y_train)\n\u001b[32m     31\u001b[39m \u001b[38;5;66;03m# 4. Make predictions on TEST data and evaluate\u001b[39;00m\n",
      "\u001b[31mTypeError\u001b[39m: SVC.__init__() got an unexpected keyword argument 'max_depth'"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "# 1. Preprocess the TRAINING data to get BERT embeddings (X_train)\n",
    "required_columns = ['tweet']  # Replace with the actual column name(s) containing the text\n",
    "\n",
    "# Preprocess training data\n",
    "X_train = preprocess_test_data(train_data, required_columns)\n",
    "# Extract the target variable from TRAINING data (replace 'label' with your actual column name)\n",
    "y_train = train_data['label']\n",
    "\n",
    "# 2. Preprocess the TESTING data to get BERT embeddings (X_test)\n",
    "X_test = preprocess_test_data(test_data, required_columns)\n",
    "# Extract the target variable from TESTING data (replace 'label' with your actual column name)\n",
    "y_test = test_data['label']\n",
    "\n",
    "# 3. Initialize and train the SVM model on TRAINING data\n",
    "from sklearn.svm import SVC\n",
    "svm_model = SVC(\n",
    "    # random_state=42, \n",
    "   \n",
    "    random_state=42,\n",
    "     kernel='linear',\n",
    "\n",
    "    # n_estimators=100,\n",
    "    max_depth=None,        # Let trees grow deep\n",
    "    min_samples_split=2,   # Minimum samples to split a node\n",
    "    min_samples_leaf=1,    # Minimum samples at a leaf node\n",
    "    class_weight='balanced'\n",
    "                )  # Changed to SVC with linear kernel\n",
    "svm_model.fit(X_train, y_train)\n",
    "\n",
    "# 4. Make predictions on TEST data and evaluate\n",
    "y_pred = svm_model.predict(X_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"SVM Model Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Print a detailed classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Print the confusion matrix\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "# 5. (Optional) Save the trained model for later use\n",
    "# joblib.dump(svm_model, 'svm_sentiment_model.pkl')\n",
    "# To load it later: loaded_model = joblib.load('svm_sentiment_model.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc83900f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# accuracy previously = 0.7105"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e236a229",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions for custom sentences: [ 1 -1  1 -1  1 -1]\n",
      "'this is the best post i have ever seen in my life this is fantastic' -> Happy (1)\n",
      "'this is the worst post i have ever seen in my life this is bad' -> Sad (-1)\n",
      "'i am happy today because i got a new job' -> Happy (1)\n",
      "'i am sad today because i lost my job' -> Sad (-1)\n",
      "'this movie is ok to watch once but not great' -> Happy (1)\n",
      "'the food was terrible and the service was worse' -> Sad (-1)\n"
     ]
    }
   ],
   "source": [
    "# Create a DataFrame from your list\n",
    "custom_test_data = pd.DataFrame({\n",
    "    'tweet': [\"this is the best post i have ever seen in my life this is fantastic\",\n",
    "              \"this is the worst post i have ever seen in my life this is bad\",\n",
    "              \"i am happy today because i got a new job\",\n",
    "              \"i am sad today because i lost my job\",\n",
    "              \"this movie is ok to watch once but not great\",\n",
    "              \"the food was terrible and the service was worse\",]\n",
    "})\n",
    "\n",
    "# Preprocess the custom test data\n",
    "X_custom_test = preprocess_test_data(custom_test_data, required_columns)\n",
    "\n",
    "# Make predictions on the custom test data\n",
    "y_custom_pred = svm_model.predict(X_custom_test)\n",
    "print(\"Predictions for custom sentences:\", y_custom_pred)\n",
    "\n",
    "# Optional: Print the sentences with their predictions\n",
    "for sentence, prediction in zip(custom_test_data['tweet'], y_custom_pred):\n",
    "    sentiment = \"Happy\" if prediction == 1 else \"Sad\"\n",
    "    print(f\"'{sentence}' -> {sentiment} ({prediction})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b85f1399",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
